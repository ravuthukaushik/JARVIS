{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4YnZ0CAY3oT",
        "outputId": "f40016bb-a76b-4caf-f6f9-d9f2c1a931e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['(656) 678-7890', '132-123-3455', '333-333-3333']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Question 1\n",
        "import re\n",
        "\n",
        "text='''hi my bjhvtvsjc  sbcsjcbno 789 345 vbj (656) 678-7890   sdgvcu 23 bcsjdb444 132-123-3455 shc333-333-3333'''\n",
        "\n",
        "pattern = r'\\(\\d{3}\\) \\d{3}-\\d{4}|\\d{3}-\\d{3}-\\d{4}'\n",
        "\n",
        "phone_numbers = re.findall(pattern, text)\n",
        "\n",
        "phone_numbers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"SpaCy is a powerful NLP library for tokenization.\")\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"Token: '{token.text}', is followed by whitespace: {token.whitespace_ !=''}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSD6UdVCZb8U",
        "outputId": "9fdf5426-6848-48e3-9a38-e7c9b1ec3902"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: 'SpaCy', is followed by whitespace: True\n",
            "Token: 'is', is followed by whitespace: True\n",
            "Token: 'a', is followed by whitespace: True\n",
            "Token: 'powerful', is followed by whitespace: True\n",
            "Token: 'NLP', is followed by whitespace: True\n",
            "Token: 'library', is followed by whitespace: True\n",
            "Token: 'for', is followed by whitespace: True\n",
            "Token: 'tokenization', is followed by whitespace: False\n",
            "Token: '.', is followed by whitespace: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 3\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "@spacy.Language.component(\"token_counter\")\n",
        "def token_counter(doc):\n",
        "    print(f\"Token count: {len(doc)}\")\n",
        "    return doc\n",
        "\n",
        "nlp.add_pipe(\"token_counter\")\n",
        "\n",
        "doc = nlp(\"Customizing a SpaCy pipeline can be very useful.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZcxpJ0_Zwlo",
        "outputId": "7ae23823-5bbc-44ce-f2f3-406f4ab0222b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token count: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 4\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"NLP is fascinating, and SpaCy makes it even more interesting.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(f\"{'Token'}\\t\\t\\t{'POS'}\\t\\t\\t{'Dependency'}\")\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"{token.text}\\t\\t\\t{token.pos_}\\t\\t\\t{token.dep_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4Nts4BqaBrJ",
        "outputId": "03275fb8-f3c8-4351-88c5-fc481a395734"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token\t\t\tPOS\t\t\tDependency\n",
            "NLP\t\t\tPROPN\t\t\tnsubj\n",
            "is\t\t\tAUX\t\t\tROOT\n",
            "fascinating\t\t\tADJ\t\t\tacomp\n",
            ",\t\t\tPUNCT\t\t\tpunct\n",
            "and\t\t\tCCONJ\t\t\tcc\n",
            "SpaCy\t\t\tPROPN\t\t\tnsubj\n",
            "makes\t\t\tVERB\t\t\tconj\n",
            "it\t\t\tPRON\t\t\tnsubj\n",
            "even\t\t\tADV\t\t\tadvmod\n",
            "more\t\t\tADV\t\t\tadvmod\n",
            "interesting\t\t\tADJ\t\t\tccomp\n",
            ".\t\t\tPUNCT\t\t\tpunct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 5\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Elon Musk, the CEO of Tesla, was born on June 28, 1971, in Pretoria, South Africa.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(f\"{'Entity'}\\t\\t\\t{'Label'}\\t\\t\\t{'Description'}\")\n",
        "\n",
        "for entity in doc.ents:\n",
        "    print(f\"{entity.text}\\t\\t{entity.label_}\\t\\t\\t{spacy.explain(entity.label_)}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6w_OjWHaS_X",
        "outputId": "99ea8651-1606-4a9b-be36-c6e787ba1b27"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity\t\t\tLabel\t\t\tDescription\n",
            "Elon Musk\t\tPERSON\t\t\tPeople, including fictional\n",
            "Tesla\t\tORG\t\t\tCompanies, agencies, institutions, etc.\n",
            "June 28, 1971\t\tDATE\t\t\tAbsolute or relative dates or periods\n",
            "Pretoria\t\tGPE\t\t\tCountries, cities, states\n",
            "South Africa\t\tGPE\t\t\tCountries, cities, states\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "corpus = [\"Data science is amazing\", \"Machine learning is part of data science\"]\n",
        "\n",
        "# Bag of Words\n",
        "print(\"Bag of Words:\")\n",
        "vectorizer = CountVectorizer()\n",
        "vectors = vectorizer.fit_transform(corpus)\n",
        "print(pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names_out()))\n",
        "\n",
        "# TF-IDF\n",
        "print(\"\\nTF-IDF:\")\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(corpus)\n",
        "print(pd.DataFrame(tfidf_vectors.toarray(), columns=tfidf_vectorizer.get_feature_names_out()))\n",
        "\n",
        "# One hot encoding\n",
        "print(\"\\nOne Hot Encoding:\")\n",
        "words = [word for sentence in corpus for word in sentence.split()]\n",
        "words = list(set(words))\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "one_hot_vectors = one_hot_encoder.fit_transform([[word] for word in words])\n",
        "one_hot_df = pd.DataFrame(one_hot_vectors, columns=words)\n",
        "print(one_hot_df)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi4g60Tjd64C",
        "outputId": "0ac9efd9-ce14-4226-da80-303fcb7915bb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words:\n",
            "   amazing  data  is  learning  machine  of  part  science\n",
            "0        1     1   1         0        0   0     0        1\n",
            "1        0     1   1         1        1   1     1        1\n",
            "\n",
            "TF-IDF:\n",
            "    amazing      data        is  learning   machine        of      part  \\\n",
            "0  0.630099  0.448321  0.448321  0.000000  0.000000  0.000000  0.000000   \n",
            "1  0.000000  0.302873  0.302873  0.425677  0.425677  0.425677  0.425677   \n",
            "\n",
            "    science  \n",
            "0  0.448321  \n",
            "1  0.302873  \n",
            "\n",
            "One Hot Encoding:\n",
            "   part   is   of  data  science  Machine  learning  Data  amazing\n",
            "0   0.0  0.0  0.0   0.0      0.0      0.0       0.0   1.0      0.0\n",
            "1   0.0  0.0  0.0   0.0      1.0      0.0       0.0   0.0      0.0\n",
            "2   0.0  0.0  0.0   0.0      0.0      0.0       1.0   0.0      0.0\n",
            "3   0.0  0.0  0.0   1.0      0.0      0.0       0.0   0.0      0.0\n",
            "4   0.0  0.0  0.0   0.0      0.0      0.0       0.0   0.0      1.0\n",
            "5   0.0  1.0  0.0   0.0      0.0      0.0       0.0   0.0      0.0\n",
            "6   0.0  0.0  0.0   0.0      0.0      1.0       0.0   0.0      0.0\n",
            "7   1.0  0.0  0.0   0.0      0.0      0.0       0.0   0.0      0.0\n",
            "8   0.0  0.0  1.0   0.0      0.0      0.0       0.0   0.0      0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Empit8t30NsA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}